import pandas as pd
import numpy as np
from typing import Optional, Dict, List
from utils.logger import setup_logger

logger = setup_logger("preprocessor")

class DataPreprocessor:
    """
    æ•°æ®é¢„å¤„ç†ç®¡é“,åŒ…å«:
    1. æ—¶é—´æˆ³å¯¹é½ä¸å¡«å……
    2. å¼‚å¸¸å€¼å»å™ª
    3. Log Returns è®¡ç®—
    4. Z-Score æ ‡å‡†åŒ–
    """
    
    def __init__(self):
        self.scaler_stats: Optional[Dict[str, Dict[str, float]]] = None
        
    def clean_pipeline(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:
        """
        å®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹
        
        Args:
            df: åŸå§‹ OHLCV æ•°æ®
            is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ (å½±å“æ ‡å‡†åŒ–è¡Œä¸º)
            
        Returns:
            æ¸…æ´—åçš„æ•°æ®
        """
        logger.info("ğŸ§¹ å¼€å§‹æ•°æ®é¢„å¤„ç†æµç¨‹...")
        
        # 1. æ—¶é—´æˆ³å¯¹é½
        df = self.align_timestamps(df)
        logger.info(f"  âœ“ æ—¶é—´æˆ³å¯¹é½å®Œæˆ: {len(df)} è¡Œ")
        
        # 2. ç¼ºå¤±å€¼å¡«å……
        df = self.fill_missing(df)
        logger.info(f"  âœ“ ç¼ºå¤±å€¼å¡«å……å®Œæˆ: {len(df)} è¡Œ")
        
        # 3. å¼‚å¸¸å€¼å»é™¤
        df = self.remove_outliers(df)
        logger.info(f"  âœ“ å¼‚å¸¸å€¼å»é™¤å®Œæˆ: {len(df)} è¡Œ")
        
        # 4. Log Returns (åœ¨ç‰¹å¾å·¥ç¨‹ä¹‹å‰è®¡ç®—åŸºç¡€æ”¶ç›Šç‡)
        df = self.add_log_returns(df)
        logger.info(f"  âœ“ Log Returns è®¡ç®—å®Œæˆ")
        
        logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        return df
    
    def align_timestamps(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤šæ ‡çš„æ—¶é—´æˆ³å¯¹é½,ç¡®ä¿æ‰€æœ‰æ ‡çš„åœ¨ç›¸åŒæ—¶é—´ç‚¹éƒ½æœ‰æ•°æ®
        
        ç­–ç•¥:
        - æŒ‰ symbol åˆ†ç»„
        - ä½¿ç”¨å®Œæ•´æ—¶é—´åºåˆ— reindex
        - å¯¹é½åçš„ç¼ºå¤±å€¼å°†åœ¨ fill_missing ä¸­å¤„ç†
        """
        if 'symbol' not in df.columns:
            logger.warning("  âš ï¸  æ•°æ®ä¸­æ²¡æœ‰ symbol åˆ—,è·³è¿‡æ—¶é—´å¯¹é½")
            return df
        
        # è·å–æ‰€æœ‰æ—¶é—´æˆ³çš„å¹¶é›†
        all_timestamps = df['timestamp'].unique()
        all_timestamps = pd.Series(all_timestamps).sort_values().values
        
        aligned_groups = []
        for symbol, group in df.groupby('symbol'):
            # åˆ›å»ºå®Œæ•´æ—¶é—´ç´¢å¼•
            group = group.set_index('timestamp').sort_index()
            group = group.reindex(all_timestamps)
            group['symbol'] = symbol
            aligned_groups.append(group.reset_index().rename(columns={'index': 'timestamp'}))
        
        df_aligned = pd.concat(aligned_groups, ignore_index=True)
        return df_aligned
    
    def fill_missing(self, df: pd.DataFrame, max_consecutive_na: int = 5) -> pd.DataFrame:
        """
        æ™ºèƒ½å¡«å……ç¼ºå¤±å€¼
        
        ç­–ç•¥:
        - OHLCV: å‰å‘å¡«å…… (ffill) - å‡è®¾ä»·æ ¼åœ¨ç¼ºå¤±æœŸé—´ä¿æŒä¸å˜
        - å…¶ä»–åˆ—: çº¿æ€§æ’å€¼
        - è¶…è¿‡ max_consecutive_na çš„è¿ç»­ç¼ºå¤±: åˆ é™¤
        """
        ohlcv_cols = ['open', 'high', 'low', 'close', 'volume']
        
        def fill_group(group):
            # OHLCV å‰å‘å¡«å……
            for col in ohlcv_cols:
                if col in group.columns:
                    group[col] = group[col].ffill()
            
            # å…¶ä»–æ•°å€¼åˆ—çº¿æ€§æ’å€¼
            numeric_cols = group.select_dtypes(include=[np.number]).columns
            other_cols = [c for c in numeric_cols if c not in ohlcv_cols]
            for col in other_cols:
                group[col] = group[col].interpolate(method='linear', limit=max_consecutive_na)
            
            return group
        
        if 'symbol' in df.columns:
            df = df.groupby('symbol', group_keys=False).apply(fill_group)
        else:
            df = fill_group(df)
        
        # åˆ é™¤ä»ç„¶å­˜åœ¨çš„ NaN (è¶…è¿‡æœ€å¤§è¿ç»­ç¼ºå¤±)
        initial_rows = len(df)
        df = df.dropna(subset=ohlcv_cols)
        dropped = initial_rows - len(df)
        if dropped > 0:
            logger.info(f"  âš ï¸  åˆ é™¤äº† {dropped} è¡Œæ— æ³•å¡«å……çš„æ•°æ®")
        
        return df
    
    def remove_outliers(self, df: pd.DataFrame, method: str = 'iqr') -> pd.DataFrame:
        """
        æ£€æµ‹å¹¶ç§»é™¤å¼‚å¸¸å€¼
        
        æ–¹æ³•:
        1. IQR æ–¹æ³•: Q1 - 1.5*IQR, Q3 + 1.5*IQR
        2. é—ªå´©æ£€æµ‹: å•å‘¨æœŸè·Œå¹… > 20%
        3. é”™è¯¯æŠ¥ä»·: volume = 0 ä½†ä»·æ ¼å˜åŠ¨
        """
        initial_rows = len(df)
        
        def detect_outliers(group):
            # 1. é—ªå´©æ£€æµ‹ (å•å‘¨æœŸè·Œå¹… > 20%)
            returns = group['close'].pct_change()
            flash_crash = returns < -0.20
            
            # 2. é”™è¯¯æŠ¥ä»· (volume = 0 ä½†ä»·æ ¼å˜åŠ¨)
            price_change = group['close'].diff().abs() > 0
            zero_volume = group['volume'] == 0
            bad_quotes = price_change & zero_volume
            
            # 3. IQR å¼‚å¸¸å€¼æ£€æµ‹ (é’ˆå¯¹ volume å’Œ returns)
            if method == 'iqr':
                # Volume å¼‚å¸¸
                Q1_vol = group['volume'].quantile(0.25)
                Q3_vol = group['volume'].quantile(0.75)
                IQR_vol = Q3_vol - Q1_vol
                volume_outliers = (group['volume'] < Q1_vol - 1.5 * IQR_vol) | \
                                 (group['volume'] > Q3_vol + 1.5 * IQR_vol)
                
                # Returns å¼‚å¸¸ (éœ€è¦å…ˆè®¡ç®—)
                if len(returns.dropna()) > 0:
                    Q1_ret = returns.quantile(0.25)
                    Q3_ret = returns.quantile(0.75)
                    IQR_ret = Q3_ret - Q1_ret
                    return_outliers = (returns < Q1_ret - 3 * IQR_ret) | \
                                     (returns > Q3_ret + 3 * IQR_ret)
                else:
                    return_outliers = pd.Series(False, index=group.index)
            else:
                volume_outliers = pd.Series(False, index=group.index)
                return_outliers = pd.Series(False, index=group.index)
            
            # ç»„åˆæ‰€æœ‰å¼‚å¸¸æ ‡è®°
            is_outlier = flash_crash | bad_quotes | volume_outliers | return_outliers
            return group[~is_outlier]
        
        if 'symbol' in df.columns:
            df = df.groupby('symbol', group_keys=False).apply(detect_outliers)
        else:
            df = detect_outliers(df)
        
        removed = initial_rows - len(df)
        if removed > 0:
            logger.info(f"  ğŸ—‘ï¸  ç§»é™¤äº† {removed} ä¸ªå¼‚å¸¸å€¼ ({removed/initial_rows:.2%})")
        
        return df.reset_index(drop=True)
    
    def add_log_returns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®— Log Returns (å¯¹æ•°æ”¶ç›Šç‡)
        
        ä¼˜åŠ¿:
        - æ›´å¥½çš„ç»Ÿè®¡ç‰¹æ€§ (è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ)
        - æ—¶é—´å¯åŠ æ€§: log_ret(t1->t3) = log_ret(t1->t2) + log_ret(t2->t3)
        - å¯¹ç§°æ€§: Â±10% åœ¨ log ç©ºé—´æ˜¯å¯¹ç§°çš„
        """
        def calc_log_returns(group):
            # 1 å‘¨æœŸ Log Returns
            group['log_return_1p'] = np.log(group['close'] / group['close'].shift(1))
            
            # 5 å‘¨æœŸ Log Returns
            group['log_return_5p'] = np.log(group['close'] / group['close'].shift(5))
            
            # æ›¿æ¢ inf å’Œ -inf (é™¤ä»¥ 0 çš„æƒ…å†µ)
            group['log_return_1p'] = group['log_return_1p'].replace([np.inf, -np.inf], np.nan)
            group['log_return_5p'] = group['log_return_5p'].replace([np.inf, -np.inf], np.nan)
            
            return group
        
        if 'symbol' in df.columns:
            df = df.groupby('symbol', group_keys=False).apply(calc_log_returns)
        else:
            df = calc_log_returns(df)
        
        return df
    
    def standardize_features(self, df: pd.DataFrame, 
                            feature_cols: List[str],
                            fit: bool = True) -> pd.DataFrame:
        """
        Z-Score æ ‡å‡†åŒ–ç‰¹å¾
        
        Args:
            df: æ•°æ®
            feature_cols: éœ€è¦æ ‡å‡†åŒ–çš„ç‰¹å¾åˆ—
            fit: æ˜¯å¦æ‹Ÿåˆç»Ÿè®¡é‡ (è®­ç»ƒæ—¶ True, é¢„æµ‹æ—¶ False)
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        # æ’é™¤ä¸éœ€è¦æ ‡å‡†åŒ–çš„åˆ—
        exclude_cols = ['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'volume']
        exclude_cols += [c for c in df.columns if c.startswith('target_')]
        
        cols_to_scale = [c for c in feature_cols if c not in exclude_cols and c in df.columns]
        
        if fit:
            # è®­ç»ƒæ¨¡å¼: è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡
            self.scaler_stats = {}
            for col in cols_to_scale:
                mean = df[col].mean()
                std = df[col].std()
                self.scaler_stats[col] = {'mean': mean, 'std': std}
                
                # æ ‡å‡†åŒ–
                if std > 1e-8:  # é¿å…é™¤ä»¥ 0
                    df[col] = (df[col] - mean) / std
                else:
                    logger.warning(f"  âš ï¸  åˆ— {col} æ ‡å‡†å·®æ¥è¿‘ 0, è·³è¿‡æ ‡å‡†åŒ–")
            
            logger.info(f"  ğŸ“Š æ ‡å‡†åŒ–äº† {len(cols_to_scale)} ä¸ªç‰¹å¾")
        else:
            # é¢„æµ‹æ¨¡å¼: ä½¿ç”¨è®­ç»ƒæ—¶çš„ç»Ÿè®¡é‡
            if self.scaler_stats is None:
                raise ValueError("é¢„æµ‹æ¨¡å¼ä¸‹å¿…é¡»å…ˆåŠ è½½ scaler_stats (è°ƒç”¨ load_scaler_stats)")
            
            for col in cols_to_scale:
                if col in self.scaler_stats:
                    mean = self.scaler_stats[col]['mean']
                    std = self.scaler_stats[col]['std']
                    if std > 1e-8:
                        df[col] = (df[col] - mean) / std
                else:
                    logger.warning(f"  âš ï¸  åˆ— {col} æ²¡æœ‰ä¿å­˜çš„ç»Ÿè®¡é‡, è·³è¿‡æ ‡å‡†åŒ–")
        
        return df
    
    def save_scaler_stats(self, filepath: str):
        """ä¿å­˜æ ‡å‡†åŒ–ç»Ÿè®¡é‡"""
        if self.scaler_stats is None:
            raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„ç»Ÿè®¡é‡")
        
        import joblib
        joblib.dump(self.scaler_stats, filepath)
        logger.info(f"ğŸ’¾ æ ‡å‡†åŒ–ç»Ÿè®¡é‡å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_scaler_stats(self, filepath: str):
        """åŠ è½½æ ‡å‡†åŒ–ç»Ÿè®¡é‡"""
        import joblib
        self.scaler_stats = joblib.load(filepath)
        logger.info(f"ğŸ“¦ æ ‡å‡†åŒ–ç»Ÿè®¡é‡å·²åŠ è½½: {filepath}")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from datetime import datetime, timedelta
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dates = pd.date_range(start='2024-01-01', periods=100, freq='1H')
    symbols = ['AAPL', 'GOOGL']
    
    data = []
    for sym in symbols:
        for i, d in enumerate(dates):
            # æ¨¡æ‹Ÿä¸€äº›ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
            if i % 20 == 0:  # æ¯ 20 ä¸ªå‘¨æœŸç¼ºå¤±ä¸€æ¬¡
                continue
            
            price = 100 + np.random.randn() * 5
            if i == 50:  # æ¨¡æ‹Ÿé—ªå´©
                price = 50
            
            data.append({
                'timestamp': d,
                'symbol': sym,
                'open': price,
                'high': price + abs(np.random.randn()),
                'low': price - abs(np.random.randn()),
                'close': price,
                'volume': max(0, 1000 + np.random.randn() * 100)
            })
    
    df = pd.DataFrame(data)
    print(f"åŸå§‹æ•°æ®: {len(df)} è¡Œ")
    
    # æµ‹è¯•é¢„å¤„ç†
    preprocessor = DataPreprocessor()
    df_clean = preprocessor.clean_pipeline(df, is_training=True)
    print(f"\næ¸…æ´—åæ•°æ®: {len(df_clean)} è¡Œ")
    print(f"\nLog Returns ç»Ÿè®¡:")
    print(df_clean['log_return_1p'].describe())
