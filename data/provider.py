import os
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, List, Union
from alpaca.data.historical import StockHistoricalDataClient
from alpaca.data.requests import StockBarsRequest
from alpaca.data.timeframe import TimeFrame, TimeFrameUnit
from alpaca.data.enums import DataFeed

class DataProvider:
    @staticmethod
    def get_tf_string(tf: TimeFrame) -> str:
        """
        å°† Alpaca TimeFrame è½¬æ¢ä¸ºä¸šç•Œé€šç”¨å­—ç¬¦ä¸² (å¦‚ 1d, 15m, 1h)
        """
        unit_map = {
            TimeFrameUnit.Minute: 'm',
            TimeFrameUnit.Hour: 'h',
            TimeFrameUnit.Day: 'd',
            TimeFrameUnit.Week: 'w',
            TimeFrameUnit.Month: 'M'
        }
        unit_str = unit_map.get(tf.unit, 'u')
        return f"{tf.amount}{unit_str}"

    def __init__(self, api_key: Optional[str] = None, secret_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("ALPACA_API_KEY")
        self.secret_key = secret_key or os.getenv("ALPACA_SECRET_KEY")
        
        if not self.api_key or not self.secret_key:
            raise ValueError("Alpaca API Key and Secret Key must be provided or set as environment variables.")
        
        self.client = StockHistoricalDataClient(self.api_key, self.secret_key)
        
        # ç¼“å­˜é…ç½®
        self.cache_dir = "data/cache"
        os.makedirs(self.cache_dir, exist_ok=True)

    def fetch_bars(self, symbols: Union[str, List[str]], timeframe: TimeFrame, start: datetime, end: Optional[datetime] = None, use_cache: bool = True, use_redis: bool = False) -> pd.DataFrame:
        """
        Fetch historical bars with local caching (File or Redis).
        
        Args:
            use_redis: å¦‚æœä¸º True,åˆ™å°è¯•ä½¿ç”¨ Redis è¿›è¡Œå¢é‡æ›´æ–°
        """
        if end is None:
            # å¼ºåˆ¶å°†æœ¬åœ°æ—¶é—´è§†ä¸ºåŒ—äº¬æ—¶é—´ (Asia/Shanghai)
            # è¿™æ˜¯ä¸ºäº†è§£å†³ç”¨æˆ·ç³»ç»Ÿæ—¶åŒºè®¾ç½®ä¸æ­£ç¡®çš„é—®é¢˜
            import pytz
            local_naive = datetime.now()
            beijing_tz = pytz.timezone('Asia/Shanghai')
            # å‡å®šæœ¬åœ°æ—¶é—´å°±æ˜¯åŒ—äº¬æ—¶é—´
            local_aware = beijing_tz.localize(local_naive)
            # è½¬ä¸º UTC ä¾›åç»­ä½¿ç”¨
            end_aware = local_aware.astimezone(pytz.utc)
            
            # æ£€æŸ¥ start æ˜¯å¦ä¸º Naive (é€šå¸¸æ„å‘³ç€æ˜¯ NY Time)
            if start.tzinfo is None:
                # å°† end è½¬ä¸º Naive NY Time ä»¥åŒ¹é… start
                ny_tz = pytz.timezone('America/New_York')
                end = end_aware.astimezone(ny_tz).replace(tzinfo=None)
            else:
                end = end_aware

        # --- Redis å¢é‡æ›´æ–°é€»è¾‘ ---
        if use_redis:
            try:
                from data.redis_manager import RedisDataManager
                redis_mgr = RedisDataManager()
                
                # ç»Ÿä¸€å¤„ç†å•ä¸ªå’Œå¤šä¸ªæ ‡çš„
                sym_list = [symbols] if isinstance(symbols, str) else symbols
                
                # 1. æ‰¹é‡æ£€æŸ¥æ‰€æœ‰æ ‡çš„çš„æœ€æ–°æ—¶é—´
                # ä¸ºäº†æ‰¹é‡è°ƒç”¨çš„æ•ˆç‡,æˆ‘ä»¬æ‰¾åˆ°"æœ€æ—§"çš„æœ€æ–°æ—¶é—´,ä¸€æ¬¡æ€§æ‹‰å–æ‰€æœ‰å¢é‡
                # (è™½ç„¶è¿™å¯èƒ½ä¼šé‡å¤æ‹‰å–ä¸€äº›æ•°æ®,ä½†æ¯”å‘ N ä¸ª API è¯·æ±‚è¦å¿«å¾—å¤šä¸”çœé¢åº¦)
                active_start_time = start
                
                # æ£€æŸ¥æ¯ä¸ªæ ‡çš„åœ¨ Redis çš„æœ€æ–°æ—¶é—´
                latest_timestamps = []
                for sym in sym_list:
                    ts = redis_mgr.get_latest_timestamp(sym, timeframe)
                    if ts:
                        latest_timestamps.append(ts)
                    else:
                        latest_timestamps.append(None)
                
                # å¦‚æœæ‰€æœ‰æ ‡çš„éƒ½æœ‰ç¼“å­˜æ•°æ®,æ‰¾åˆ°æœ€æ—©çš„ä¸€ä¸ªä½œä¸ºå¢é‡èµ·ç‚¹
                if all(ts is not None for ts in latest_timestamps):
                    min_ts = min(latest_timestamps)
                    
                    # è½¬æ¢ min_ts ä¸º ET ç”¨äºæ˜¾ç¤º
                    import pytz
                    ny_tz = pytz.timezone('America/New_York')
                    show_min_ts = min_ts.astimezone(ny_tz) if min_ts.tzinfo else pytz.utc.localize(min_ts).astimezone(ny_tz)
                    show_start_chk = start.astimezone(ny_tz) if start.tzinfo else pytz.utc.localize(start).astimezone(ny_tz)

                    if min_ts >= start:
                         # å¢é‡èµ·ç‚¹: æœ€æ—©çš„ç¼“å­˜æ—¶é—´ + 1åˆ†é’Ÿ (é˜²æ­¢é‡å )
                        active_start_time = min_ts + timedelta(minutes=1)
                        print(f"ğŸ”„ Redis æ‰¹é‡å¢é‡: æœ¬åœ°æ•°æ®å‡æ–°äº {show_min_ts.strftime('%Y-%m-%d %H:%M:%S')} ET, ä»…æ‹‰å–å¢é‡...")
                    else:
                        print(f"ğŸ“¥ Redis æ•°æ®è¾ƒæ—§ (éƒ¨åˆ†æ—§äº {show_start_chk.strftime('%Y-%m-%d %H:%M:%S')} ET), æ‹‰å–å®Œæ•´å†å²...")
                else:
                    print(f"ğŸ“¥ Redis éƒ¨åˆ†æ ‡çš„ç¼ºæ•°æ®, æ‹‰å–å®Œæ•´å†å²...")

                # 2. ä» API æ‰¹é‡æ‹‰å–æ•°æ® (å¦‚æœéœ€è¦)
                if active_start_time < end:
                    # è½¬æ¢æ˜¾ç¤ºæ—¶é—´ä¸º ET
                    import pytz
                    ny_tz = pytz.timezone('America/New_York')
                    
                    show_start = active_start_time
                    if show_start.tzinfo is None:
                        # Assumed to be Naive NY Time (based on project convention)
                        import pytz
                        ny_tz = pytz.timezone('America/New_York')
                        show_start = ny_tz.localize(show_start)
                    else:
                        show_start = show_start.astimezone(ny_tz)
                        
                    show_end = end
                    if show_end.tzinfo is None:
                        show_end = pytz.utc.localize(show_end).astimezone(ny_tz)
                    else:
                        show_end = show_end.astimezone(ny_tz)
                        
                    print(f"DEBUG TIME: Now(UTC)={end} | End(ET)={show_end}")
                    print(f"â¬‡ï¸  Fetching batch data from API ({show_start.strftime('%Y-%m-%d %H:%M:%S')} ET -> {show_end.strftime('%Y-%m-%d %H:%M:%S')} ET)...")
                    request_params = StockBarsRequest(
                        symbol_or_symbols=sym_list,
                        timeframe=timeframe,
                        start=active_start_time,
                        end=end,
                        feed=DataFeed.IEX
                    )
                    try:
                        bars = self.client.get_stock_bars(request_params)
                        new_df = bars.df
                        print(f"âœ… API returned {len(new_df)} rows of data.")
                        
                        if not new_df.empty:
                            print(f"ğŸ” API Data Preview:\n{new_df.iloc[[0, -1]][['timestamp']] if 'timestamp' in new_df.columns else new_df.index[[0, -1]]}")

                            # ç»Ÿä¸€æ ¼å¼å¤„ç†
                            if isinstance(new_df.index, pd.MultiIndex):
                                new_df = new_df.reset_index()
                            else:
                                new_df = new_df.reset_index()
                                # å¦‚æœAPIè¿”å›å•æ ‡çš„æ ¼å¼ä½†æˆ‘ä»¬è¯·æ±‚çš„æ˜¯åˆ—è¡¨(æç«¯æƒ…å†µ),è¡¥å…¨ symbol
                                if 'symbol' not in new_df.columns and len(sym_list) == 1:
                                     new_df['symbol'] = sym_list[0]

                            if 'timestamp' in new_df.columns:
                                new_df['timestamp'] = pd.to_datetime(new_df['timestamp']).dt.tz_convert('America/New_York').dt.tz_localize(None)
                            
                            # 3. æŒ‰æ ‡çš„åˆ†ç»„å¹¶ä¿å­˜åˆ° Redis
                            grouped = new_df.groupby('symbol')
                            for sym, group in grouped:
                                redis_mgr.save_bars(group, sym, timeframe)
                                
                    except Exception as e:
                        import traceback
                        print(f"âš ï¸  Batch fetch failed (maybe no new data): {e}")
                        print(traceback.format_exc())

                # 4. ä» Redis ç»„è£…å®Œæ•´æ•°æ®é›†è¿”å›
                import pytz
                ny_tz = pytz.timezone('America/New_York')
                
                show_start_full = start
                if show_start_full.tzinfo is None:
                     show_start_full = pytz.utc.localize(show_start_full).astimezone(ny_tz)
                else:
                     show_start_full = show_start_full.astimezone(ny_tz)

                show_end_full = end
                if show_end_full.tzinfo is None:
                     show_end_full = pytz.utc.localize(show_end_full).astimezone(ny_tz)
                else:
                     show_end_full = show_end_full.astimezone(ny_tz)

                print(f"ğŸ“¦ Loading full batch dataset from Redis ({show_start_full.strftime('%Y-%m-%d %H:%M:%S')} ET -> {show_end_full.strftime('%Y-%m-%d %H:%M:%S')} ET)...")
                all_data = []
                for sym in sym_list:
                    df_sym = redis_mgr.get_bars(sym, timeframe, start, end)
                    if not df_sym.empty:
                        df_sym['symbol'] = sym
                        all_data.append(df_sym)
                
                if all_data:
                    full_df = pd.concat(all_data, ignore_index=True)
                    # ç¡®ä¿æ ¼å¼ç¬¦åˆé¢„æœŸ (symbol, timestamp) MultiIndex æˆ– Column
                    # è¿™é‡Œè¿”å› flat DataFrame, è®©è°ƒç”¨è€…å¤„ç†
                    return full_df
                else:
                    return pd.DataFrame()
                
            except ImportError:
                print("âš ï¸  Redis dependencies not installed. Falling back to file cache.")
            except Exception as e:
                print(f"âš ï¸  Redis batch operation failed: {e}. Falling back to file cache.")

        # --- åŸæœ‰çš„æ–‡ä»¶ç¼“å­˜é€»è¾‘ (Fallback) ---
        # 1. ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
        # æ ¼å¼: timeframe_start_end_hash.parquet
        sym_str = symbols if isinstance(symbols, str) else "_".join(sorted(symbols))
        # å¦‚æœ symbol å¤ªå¤šï¼Œä½¿ç”¨ hash é¿å…æ–‡ä»¶åè¿‡é•¿
        if len(sym_str) > 50:
            import hashlib
            sym_str = hashlib.md5(sym_str.encode()).hexdigest()
            
        tf_str = self.get_tf_string(timeframe)
        start_str = start.strftime('%Y%m%d')
        end_str = end.strftime('%Y%m%d')
        
        cache_file = os.path.join(self.cache_dir, f"{sym_str}_{tf_str}_{start_str}_{end_str}.parquet")
        
        # 2. å°è¯•è¯»å–ç¼“å­˜
        if use_cache and os.path.exists(cache_file):
            try:
                print(f"ğŸ“¦ Loading cached data from {cache_file}...")
                df = pd.read_parquet(cache_file)
                return df
            except Exception as e:
                print(f"âš ï¸  Cache load failed, fetching from API: {e}")

        # 3. ä» API è·å–æ•°æ®
        print(f"â¬‡ï¸  Fetching data from Alpaca API ([{sym_str}] {tf_str})...")
        request_params = StockBarsRequest(
            symbol_or_symbols=symbols,
            timeframe=timeframe,
            start=start,
            end=end,
            feed=DataFeed.IEX
        )
        
        bars = self.client.get_stock_bars(request_params)
        df = bars.df
        
        # å¤„ç†å¤šç´¢å¼•
        if isinstance(df.index, pd.MultiIndex):
            df = df.reset_index()
        else:
            # å¦‚æœæ˜¯å•æ ‡çš„ï¼Œæ‰‹åŠ¨åŠ ä¸Š symbol åˆ—ä¿æŒæ ¼å¼ç»Ÿä¸€
            df = df.reset_index()
            if isinstance(symbols, str):
                df['symbol'] = symbols
        
        # ç¡®ä¿ timestamp åˆ—è½¬æ¢ä¸ºç¾ä¸œæ—¶é—´ (America/New_York)
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_convert('America/New_York').dt.tz_localize(None)
            
        # 4. ä¿å­˜ç¼“å­˜
        if use_cache and not df.empty:
            try:
                df.to_parquet(cache_file, index=False)
                print(f"ğŸ’¾ Data cached to {cache_file}")
            except Exception as e:
                print(f"âš ï¸  Cache save failed: {e}")
            
        return df

if __name__ == "__main__":
    # Example usage (will fail if keys not set)
    try:
        provider = DataProvider()
        # Fetch last 30 days of daily data for QQQ
        end = datetime.now()
        start = end - timedelta(days=30)
        data = provider.fetch_bars("QQQ", TimeFrame.Day, start, end)
        print(data.head())
    except Exception as e:
        print(f"Error fetching data: {e}")
