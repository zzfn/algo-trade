"""
è®­ç»ƒç¨³å¥æ€§ä¼˜åŒ–æ¨¡å— (Training Robustness)

æ ¸å¿ƒåŠŸèƒ½:
1. Purged Cross-Validation - é˜²æ­¢ç‰¹å¾é‡å å¯¼è‡´çš„ä¿¡æ¯æ³„éœ²
2. Sample Weighting - æ—¶é—´è¡°å‡ + å›æŠ¥åŠ æƒ
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from typing import List, Tuple
from dataclasses import dataclass
from sklearn.metrics import f1_score
import warnings

warnings.filterwarnings('ignore')


@dataclass
class RobustTrainConfig:
    """ç¨³å¥è®­ç»ƒé…ç½®"""
    # Purged CV é…ç½®
    n_splits: int = 5             # äº¤å‰éªŒè¯æŠ˜æ•°
    purge_periods: int = 5        # æ¸…é™¤çš„å‘¨æœŸæ•° (é˜²æ­¢å‰ç»åå·®)
    embargo_periods: int = 3      # ç¦æ­¢ä½¿ç”¨çš„å‘¨æœŸæ•°
    
    # æ ·æœ¬åŠ æƒé…ç½®
    use_time_decay: bool = True   # æ˜¯å¦å¯ç”¨æ—¶é—´è¡°å‡
    decay_half_life_days: int = 90  # åŠè¡°æœŸ (å¤©)
    
    use_return_weight: bool = True  # æ˜¯å¦å¯ç”¨å›æŠ¥åŠ æƒ
    extreme_quantile: float = 0.9   # æç«¯å›æŠ¥åˆ†ä½æ•°
    extreme_boost: float = 1.5      # æç«¯æ ·æœ¬æƒé‡å€æ•°


class RobustTrainer:
    """
    ç¨³å¥æ¨¡å‹è®­ç»ƒå™¨
    
    è‡ªåŠ¨é›†æˆ:
    - Purged Cross-Validation (é˜²æ­¢ä¿¡æ¯æ³„éœ²)
    - Sample Weighting (æ—¶é—´è¡°å‡ + å›æŠ¥åŠ æƒ)
    """
    
    def __init__(self, config: RobustTrainConfig = None):
        self.config = config or RobustTrainConfig()
        self.model = None
        self.cv_scores = []
    
    def compute_sample_weights(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        timestamp_col: str = 'timestamp'
    ) -> np.ndarray:
        """è®¡ç®—æ ·æœ¬æƒé‡ (æ—¶é—´è¡°å‡ + å›æŠ¥åŠ æƒ)"""
        n = len(df)
        weights = np.ones(n)
        
        # 1. æ—¶é—´è¡°å‡æƒé‡: è¶Šè¿‘æœŸçš„æ ·æœ¬æƒé‡è¶Šé«˜
        if self.config.use_time_decay and timestamp_col in df.columns:
            timestamps = pd.to_datetime(df[timestamp_col])
            most_recent = timestamps.max()
            days_ago = (most_recent - timestamps).dt.total_seconds() / 86400
            
            decay_lambda = np.log(2) / self.config.decay_half_life_days
            weights *= np.exp(-decay_lambda * days_ago)
        
        # 2. å›æŠ¥åŠ æƒ: æç«¯æ ·æœ¬è·å¾—æ›´é«˜æƒé‡
        if self.config.use_return_weight and target_col in df.columns:
            abs_target = np.abs(df[target_col])
            threshold = np.percentile(abs_target, self.config.extreme_quantile * 100)
            extreme_mask = abs_target > threshold
            weights[extreme_mask] *= self.config.extreme_boost
        
        # å½’ä¸€åŒ–
        weights = weights * (n / weights.sum())
        return weights
    
    def purged_cv_split(
        self, 
        df: pd.DataFrame, 
        timestamp_col: str = 'timestamp'
    ) -> List[Tuple[np.ndarray, np.ndarray]]:
        """ç”Ÿæˆ Purged K-Fold åˆ†å‰² (é˜²æ­¢ä¿¡æ¯æ³„éœ²)"""
        df = df.copy()
        df['_index'] = np.arange(len(df))
        
        unique_times = df[timestamp_col].sort_values().unique()
        n_times = len(unique_times)
        fold_size = n_times // self.config.n_splits
        
        splits = []
        
        for i in range(self.config.n_splits):
            test_start = i * fold_size
            test_end = (i + 1) * fold_size if i < self.config.n_splits - 1 else n_times
            test_times = unique_times[test_start:test_end]
            
            # Purge: ç§»é™¤ä¸æµ‹è¯•é›†ç›¸é‚»çš„è®­ç»ƒæ ·æœ¬
            purge_start = max(0, test_start - self.config.purge_periods)
            embargo_end = min(n_times, test_end + self.config.embargo_periods)
            
            train_times = np.concatenate([
                unique_times[:purge_start],
                unique_times[embargo_end:]
            ])
            
            train_mask = df[timestamp_col].isin(train_times)
            test_mask = df[timestamp_col].isin(test_times)
            
            train_idx = df.loc[train_mask, '_index'].values
            test_idx = df.loc[test_mask, '_index'].values
            
            if len(train_idx) > 0 and len(test_idx) > 0:
                splits.append((train_idx, test_idx))
        
        return splits
    
    def train_ranker(
        self,
        df: pd.DataFrame,
        feature_cols: List[str],
        target_col: str,
        timestamp_col: str = 'timestamp',
        model_params: dict = None
    ) -> Tuple[lgb.LGBMRanker, dict]:
        """
        è®­ç»ƒæ’åºæ¨¡å‹ (LGBMRanker)
        
        Returns:
            (model, results) å…ƒç»„
        """
        print("ğŸ›¡ï¸ ç¨³å¥è®­ç»ƒ: LGBMRanker (Purged CV + æ ·æœ¬åŠ æƒ)")
        
        df = df.sort_values(timestamp_col).reset_index(drop=True)
        weights = self.compute_sample_weights(df, target_col, timestamp_col)
        splits = self.purged_cv_split(df, timestamp_col)
        
        params = model_params or {
            "objective": "lambdarank", "metric": "ndcg",
            "num_leaves": 31, "learning_rate": 0.05, "n_estimators": 100,
            "subsample": 0.8, "colsample_bytree": 0.8,
            "random_state": 42, "verbosity": -1
        }
        
        cv_scores = []
        
        for fold, (train_idx, test_idx) in enumerate(splits):
            X_train, y_train = df.iloc[train_idx][feature_cols], df.iloc[train_idx][target_col]
            X_test, y_test = df.iloc[test_idx][feature_cols], df.iloc[test_idx][target_col]
            
            train_groups = df.iloc[train_idx].groupby(timestamp_col).size().tolist()
            test_groups = df.iloc[test_idx].groupby(timestamp_col).size().tolist()
            
            model = lgb.LGBMRanker(
                **params,
                label_gain=np.arange(max(y_train.max(), y_test.max()) + 1).tolist()
            )
            
            model.fit(
                X_train, y_train,
                group=train_groups,
                sample_weight=weights[train_idx],
                eval_set=[(X_test, y_test)],
                eval_group=[test_groups],
                eval_at=[1, 3],
                callbacks=[lgb.early_stopping(20, verbose=False)]
            )
            
            score = model.best_score_['valid_0'].get('ndcg@3', 0)
            cv_scores.append(score)
            print(f"  Fold {fold + 1}: NDCG@3 = {score:.4f}")
        
        # æœ€ç»ˆæ¨¡å‹: å…¨é‡æ•°æ®è®­ç»ƒ
        print("\nğŸ“Š åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        groups = df.groupby(timestamp_col).size().tolist()
        
        self.model = lgb.LGBMRanker(
            **params,
            label_gain=np.arange(int(df[target_col].max()) + 1).tolist()
        )
        self.model.fit(df[feature_cols], df[target_col], group=groups, sample_weight=weights)
        
        results = {
            'cv_scores': cv_scores,
            'mean_ndcg': np.mean(cv_scores),
            'std_ndcg': np.std(cv_scores)
        }
        
        print(f"\nâœ… å®Œæˆ! å¹³å‡ NDCG@3: {results['mean_ndcg']:.4f} Â± {results['std_ndcg']:.4f}")
        return self.model, results
    
    def train_classifier(
        self,
        df: pd.DataFrame,
        feature_cols: List[str],
        target_col: str,
        timestamp_col: str = 'timestamp',
        model_params: dict = None
    ) -> Tuple[lgb.LGBMClassifier, dict]:
        """
        è®­ç»ƒåˆ†ç±»æ¨¡å‹ (LGBMClassifier)
        
        Returns:
            (model, results) å…ƒç»„
        """
        print("ğŸ›¡ï¸ ç¨³å¥è®­ç»ƒ: LGBMClassifier (Purged CV + æ ·æœ¬åŠ æƒ)")
        
        df = df.sort_values(timestamp_col).reset_index(drop=True)
        weights = self.compute_sample_weights(df, target_col, timestamp_col)
        splits = self.purged_cv_split(df, timestamp_col)
        
        params = model_params or {
            "objective": "multiclass", "num_class": 3, "metric": "multi_logloss",
            "num_leaves": 31, "learning_rate": 0.05, "n_estimators": 200,
            "random_state": 42, "verbosity": -1
        }
        
        cv_scores = []
        
        for fold, (train_idx, test_idx) in enumerate(splits):
            X_train, y_train = df.iloc[train_idx][feature_cols], df.iloc[train_idx][target_col]
            X_test, y_test = df.iloc[test_idx][feature_cols], df.iloc[test_idx][target_col]
            
            model = lgb.LGBMClassifier(**params)
            model.fit(
                X_train, y_train,
                sample_weight=weights[train_idx],
                eval_set=[(X_test, y_test)],
                callbacks=[lgb.early_stopping(20, verbose=False)]
            )
            
            y_pred = model.predict(X_test)
            score = f1_score(y_test, y_pred, average='macro')
            cv_scores.append(score)
            print(f"  Fold {fold + 1}: F1-Macro = {score:.4f}")
        
        # æœ€ç»ˆæ¨¡å‹
        print("\nğŸ“Š åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        self.model = lgb.LGBMClassifier(**params)
        self.model.fit(df[feature_cols], df[target_col], sample_weight=weights)
        
        results = {
            'cv_scores': cv_scores,
            'mean_f1': np.mean(cv_scores),
            'std_f1': np.std(cv_scores)
        }
        
        print(f"\nâœ… å®Œæˆ! å¹³å‡ F1-Macro: {results['mean_f1']:.4f} Â± {results['std_f1']:.4f}")
        return self.model, results


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    print("=" * 50)
    print("Robust Trainer æ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    np.random.seed(42)
    dates = pd.date_range(start='2023-01-01', periods=500, freq='D')
    symbols = ['A', 'B', 'C', 'D', 'E']
    
    data = []
    for d in dates:
        for s in symbols:
            data.append({
                'timestamp': d,
                'symbol': s,
                'f1': np.random.randn(),
                'f2': np.random.randn(),
                'target': np.random.randint(0, 4)
            })
    
    df = pd.DataFrame(data)
    
    trainer = RobustTrainer()
    model, results = trainer.train_ranker(df, ['f1', 'f2'], 'target')
    print(f"\nç»“æœ: {results}")
